\documentclass{article}
\usepackage{blindtext}
\usepackage[margin=0.75in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{stackengine}
\usepackage[pdftex]{graphicx}
\usepackage{float}
\usepackage{newlfont}
\usepackage{amssymb}
\usepackage{verbatim} 
\usepackage[final]{pdfpages}
\usepackage{setspace}
\setlength{\parindent}{0pt}

\title{Multi-Armed Bandit Problem}
\author{Sunith Suresh, Lin Xiao, Ilan Man and Sanjay Hariharan}
\date{\today} 
\begin{document}
\maketitle

\section{Introduction}

The Multi-Armed Bandit Problem:

\begin{quote}
Suppose you are faced with NN slot machines (colourfully called multi-armed bandits). Each bandit has an unknown probability of distributing a prize (assume for now the prizes are the same for each bandit, only the probabilities differ). Some bandits are very generous, others not so much. Of course, you don't know what these probabilities are. By only choosing one bandit per round, our task is devise a strategy to maximize our winnings.
\end{quote}

The task is complicated by the stochastic nature of the bandits. A suboptimal bandit can return many winnings, purely by chance, which would make us believe that it is a very profitable bandit. Similarly, the best bandit can return many duds. Should we keep trying losers then, or give up?

A more troublesome problem is, if we have a found a bandit that returns pretty good results, do we keep drawing from it to maintain our pretty good score, or do we try other bandits in hopes of finding an even-better bandit? This is the exploration vs. exploitation dilemma.

\section{Multi-armed bandit review}

In Machine Learning literature, the stochastic multi-armed bandit is defined as: 

\begin{quote}
Given $K$ machines, each with an unknown probability of yielding a reward, which come from a fixed but unknown distribution parameterized by $\theta_i$, for $i \in (1, ..., K)$, and $N$ total plays, decide which machines to play in order to maximize the total reward or minimize the regret.
\end{quote}

Define $R_{N}^{*}$ and $\hat{R_{N}}$ as the actual and expected maximum rewards, respectively, after $N$ plays. Therefore the regret of a given multi-armed bandit algorithm is $R_{N}^{*} - \hat{R_{N}}$. Note that since this is a stochastic problem, we aim to minimize regret in expectation or with high probability.\\

This paper presents various approaches at minimizing the total regret. Specifically, we compare heuristic approaches to theoretically optimal approaches.\\

The common theme between the algorithms outlined in this paper are that our objective is to select the optimal machine to play at each time step, given that we want to maximize future rewards. The approach to selecting which machine to play is known as the \textit{policy} or acquisition function, denoted as $U(\alpha_i, \beta_i)$. This function balances the trade off between selecting the best machine based on previous plays and the possibility of a better machine that hasn't been selected yet. This trade off is the \textit{exploration/exploitation} dilemma.\\


\subsection{$\epsilon$-greedy}

- Sunith

\subsection{Upper Confidence Bounds}

The upper confidence bound (UCB) family of algorithms simply selects the machine with the largest upper confidence bound at each round.  The intuition is this: the more times you play a machine, the tighter the confidence bounds. So, as the number of plays for each machine increases, the uncertainty decreases, and so does the width of the confidence bound.\\

We want to know with high probability that the true expected payoff of a play $\mu_i$ is less than our prescribed upper bound.\\

The key ingredient here is using Chernoff-Heofding Bounds to find the upper bound:

$$\displaystyle \textup{P}(Y + a < \mu) \leq e^{-2na^2}$$

Assuming $K$ machines:

\begin{enumerate}
\item Play each arm once
\item At any time t $\ge$ $K$, play machine $i_t$ maximizing:
	\begin{enumerate}
		\item $\bar{x_{i}}(t) + \sqrt{\frac{2 ln (t)}{T_{i,t}}}$
		\item Over $i \in {1,...N}$ 
	\end{enumerate}
\end{enumerate}

Where $\bar{x_{i}}(t)$ is the average reward obtained from machine j and $T_{i,t}$ is the number of times machine $i$ has been played so far.\\

From out analysis of the Chernoff-Hoeffing bound above we can see that the confidence bound grows with the total number of actions we have taken but shrinks with the number of times we have tried this particular action. This ensures each andction is tried infinitely often but still balances exploration and exploitation.\\

The regret for UCB1 grows at a rate of ln n.\\

\section{Bayesian approaches}

\subsection{Bernoulli Bandit}

We formulate a simple bandit scheme involving $K$ Machines and a Bernoulli Reward (0 or 1). We play one of the N machines at every iteration of the game for T iterations. Each Machine has a latent parameter governing its reward. At every step we draw a reward from the selected machine.\\

Specifically, for each machine, we generate a probability of getting a reward of 1, $\theta_i$. We assume that $\theta_i$ is distributed as a Beta. A natural choice for the likelihood of a reward, $r_i$ is a $\text{Bernoulli}(\theta_i)$. To be clear:

$$\theta_i \sim \text{Beta}(\alpha_i, \beta_i)$$
$$r_i = \text{Reward from Machine i} \sim \text{Bernoulli}(\theta_i)$$

Exploiting the conjugacy of the Beta-Bernoulli model, the posterior distribution of getting a reward from $\text{machine}_i$, after playing for $T$ iterations is:
$$\text{Beta}(\alpha_i + R_{T}, \beta_i + T - R_{T})$$

where $R_{T} = \sum_{t=1}^{T}r_{t}$

Before beginning to play, we have no prior knowledge about either Machine's propensity to yield a positive reward. Therefore, we initially set $\alpha_i, \beta_i = 1$, which is a common objective prior for the Beta distribution.\\

\subsection{General algorithm}

The generic algorithm is as follows:

\begin{enumerate}
\item For $t = 1, ..., N$
\item U($\alpha_i$, $\beta_i$) = $i_t$  // select machine $i$ at time $t$ using the policy/acquisition function
\item $r_t \sim \text{Bernoulli}(\theta_{i})$  i  // We play Machine $i$, and get Reward $r_t$
\item If $r_t = 1: \alpha_i = \alpha_i + 1$   // if the reward was successful, increase our positive belief in machine $i$
\item else if $r_t = 0: \beta_i = \beta_i + 1$  // if we didn't get a reward, increase our negative belief (i.e. failure) in machine $i$
\item $R_t = R_t + r_t$		// add reward at time $t$ to running total
\end{enumerate}

This algorithm provides a framework for how to think about this problem in a general way. To maximize expected rewards, we must optimally choose the acquisition function U. The remaning sections discuss the acquisition function.

\subsection{Dynamic Programming}

- Sanjay

\subsection{Thompson Sampling: Hueristic approach}

 The UCB algorithms are implicitly using a probability distribution, but only one number from it: the upper confidence bound.  In Bayesian thinking, we want to use the entire probability distribution.  In the preceding post I defined pa(r)pa(r) , the probability distribution from which rewards are drawn.  That's what controls the bandit.  It would be great if we could estimate this entire distribution, but we don't need to.  Why?  Because all we care about is its mean μaμa .  What we will do is encode our uncertainty of μaμa in the probability distribution p(μa|dataa)p(μa|dataa) , and then use that probability distribution to decide when to explore and when to exploit.



\subsection{Gaussian Process and Reinforcement Learning}

- Lin
\begin{itemize}
\item assume knowledge of GPs, so no need to explain what a GP is
\item quick overview of how MAB relates to reinforcement learning
\item quick overview of how GPs relate to reinforcement learning
\item using the mechanics of GP, what the connection between GP and MAB is
\item show a chart or two like in the presentation
\item this will be multiple sections
\end{itemize}

\section{Empirical Comparisons}

\subsection{Data set}

\subsection{Charts}

- using the same data set, compare UCB vs. epsilon vs. Thompson vs. gittins vs. GP

\section{Conclusion}

\section{References}


\end{document}
