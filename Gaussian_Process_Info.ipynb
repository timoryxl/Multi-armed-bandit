{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization\n",
    "1. For t = 1,2,... do:\n",
    "2. Find $x_t$ by combining attributes of the posterior distribution in a utility function u and maximizing:\n",
    "    * $x_t = \\text{argmax}u(x|D_{1:t-1})$\n",
    "3. Sample the objective function:\n",
    "    * $y_t = f(x_t) + \\epsilon$\n",
    "4. Augment the Data $D_{1:t} = {D_{1:t-1}, (x_t, y_t)}$ and update the GP\n",
    "5. End for\n",
    "\n",
    "### Gaussian Process Prediction\n",
    "\n",
    "$$P(y_{t+1} | D_{1:t}, x_{t+1}) = N(\\mu_t(x_{t+1}), \\sigma_t^2(x_{t+1}) + \\sigma_{noise}^2)$$\n",
    "$$\\mu_t(x_{t+1}) = k^T[K + \\sigma_{noise}^2I]^{-1}y_{1:t}$$\n",
    "$$\\sigma_t^2(x_{t+1}) = k(x_{t+1}, x_{t+1}) - k^T[K + \\sigma_{noise}^2I]^{-1}k$$\n",
    "\n",
    "We should choose the next point x where the mean is high (**exploitation**) and the variance is high (**exploration**).\n",
    "\n",
    "We balance this tradeoff by choosing an **acquisition function**, which guides the optimization by determining which $x_{t+1}$ to observe next. \n",
    "\n",
    "### Acquisition Function\n",
    "\n",
    "**Probability of Improvement (PI)**\n",
    "\n",
    "$PI(x) = p(f(x) \\geq \\mu^+ + \\xi)$ = \n",
    "$\\Phi(\\frac{\\mu(x) - \\mu^+ - \\xi}{\\sigma(x)})$\n",
    "\n",
    "**Expected Improvement (EI)**\n",
    "\n",
    "$EI(x) = (\\mu(x) - \\mu^+ - \\xi)\\Phi(Z) + \\sigma(x)\\phi(Z)$\n",
    "Where $\\phi(.)$ and $\\Phi(.)$ denote the PDF and CDF of Standard Normal, respectively\n",
    "\n",
    "Z = $\\frac{\\mu(x) - \\mu^+ - \\xi}{\\sigma(x)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Dynamic Programming}\n",
    "\n",
    "We begin our analysis by constructing a simple approach to maximizing Future Expected Rewards. Consider the Bernoulli Bandit case.\n",
    "\n",
    "We designate the following:\n",
    "\n",
    "$$R_1 = \\text{Reward from Machine 1} \\sim \\text{Bernoulli}(\\theta_1)$$\n",
    "$$R_2 = \\text{Reward from Machine 2} \\sim \\text{Bernoulli}(\\theta_2)$$\n",
    "\n",
    "$$\\theta_1 \\sim \\text{Beta}(\\alpha_1, \\beta_1)$$\n",
    "$$\\theta_2 \\sim \\text{Beta}(\\alpha_2, \\beta_2)$$\n",
    "\n",
    "In the beginning, we have no prior knowledge about either Machine, or more specifically, about the latent parameter governing each Machine. Therefore, we initially set $\\alpha_1, \\alpha_2, \\beta_1, \\beta_2 = 1$. \n",
    "\n",
    "Given our goals above, the main question becomes: **How do we choose which Machine to play in each Iteration?** We will refer to this question as our utility or acquisition function $U(\\theta | X) = U(\\alpha_1, \\beta_1, \\alpha_2, \\beta_2 | X)$.\n",
    "\n",
    "As we begin playing and gathering information, we continuously update our parameters of interest based on the outcome. Specifically:\n",
    "\n",
    "1. $U(\\alpha_1, \\beta_1, \\alpha_2, \\beta_2) = j$ \n",
    "    * ( We choose Machine j )\n",
    "2. $R_j = i$  \n",
    "    * ( We play Machine j, and get Reward i )\n",
    "3. If $R_j = 1$:\n",
    "    $\\alpha_j = \\alpha_j + 1$  \n",
    "4. If $R_j = 0$:\n",
    "    $\\beta_j = \\beta_j + 1$  \n",
    "\n",
    "The idea is to update our prior \"belief\" on the success of the individual Machines based on our empirical rewards. $\\alpha$ represents \"success\", while $\\beta$ represents \"failure\"\n",
    "\n",
    "Before we discuss how to construct our Acquisition Function $U(\\theta|X)$ which will help us decide which Machine to play at each iteration, we see two interesting points:\n",
    "\n",
    "At time T = 1, we have no prior belief concerning any Machine. Thus, it will not matter which Machine we pick the first time around, though it will influence all subsequent choices.\n",
    "\n",
    "At time T = N, we are at our final trial, and thus are only concerned for the Reward on this trial. As such, we will opt for a **Greedy** approach and pick the Machine with the highest Expected Reward - max($\\frac{\\alpha_1}{\\alpha_1 + \\beta_1}, \\frac{\\alpha_2}{\\alpha_2 + \\beta_2}$)\n",
    "\n",
    "For times T = 2,...,N-1, we must choose the Machine based on the Expected Sum of Future Rewards, based on our framework above: $\\mathbb{E}(\\displaystyle\\sum_{i=1}^{n} R_i)$\n",
    "\n",
    "At time T = t, $\\mathbb{E}(\\displaystyle\\sum_{i=t}^{n} R_i|M_{1:t-1}, R_{1:t-1})$ = $\\mathbb{E}(\\displaystyle\\sum_{i=t+1}^{n} R_i|\\theta)$ + $\\mathbb{E}(R_t|\\theta)$  \n",
    "$M_{1:t}$ = Machines Chosen from times 1 to t  \n",
    "$R_{1:t}$ = Rewards Given from times 1 to t\n",
    "\n",
    "Given we perform N Trials, we construct our Algorithm in a Backwards Fashion:\n",
    "\n",
    "1. For T = N:\n",
    "    * For each node, we choose the **Greedy** option, that is, the one with the maximum expected return.\n",
    "\n",
    "2. For T = N-1\n",
    "    * Given that we know the most optimal choice for the last Trial, we select the Machine on this trial that will give us the maximum Expected **sum** of Rewards\n",
    "    * $\\mathbb{E}(R_{N-1} + R_{N})$, and we know $R_N$ from the previous iteration\n",
    "    \n",
    "3. For T = N-2,...,2:\n",
    "    * We follow the similar intuition as above, choosing the most optimal choice for the maximum Expected sum of rewards, given the optimal choice for all subseqent trials already calculated\n",
    "    * $\\mathbb{E}(\\displaystyle\\sum_{i=T}^{n} R_i)$ = $\\mathbb{E}(\\displaystyle\\sum_{i=t+1}^{n} R_i)$ + $\\mathbb{E}(R_t)$\n",
    "    \n",
    "4. For T = 1:\n",
    "    * From our above note, the first iteration does not matter, as we have no prior knowledge. We select any Machine at random.\n",
    "\n",
    "As an illustration, consider the following tree. Here we denote $f(a_1, b_1, a_2, b_2)$ as the Posterior parameters of the Machine at time $t$.\n",
    "\n",
    "\\begin{figure}[H]\n",
    "\\centering\n",
    "\\includegraphics[scale=0.5]{Dynamic_Programming_Tree.png}\n",
    "\\caption{Cumulative Regret}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
